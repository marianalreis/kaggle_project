{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9466c178",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ Startup Success Prediction ‚Äî **Notebook Otimizado**\n",
    "**Autor:** Mari (Inteli)  \n",
    "\n",
    "**Criado:** 2025-09-30 17:42  \n",
    "\n",
    "**Objetivo:** Atingir **acur√°cia ‚â• 0.80** (ideal: maior que 0.85) com modelos do `scikit-learn` (Random Forest, Gradient Boosting e HistGradientBoosting) usando *pipelines* e *hyperparameter search* (Randomized + Grid).\n",
    "\n",
    "## Checklist dos Crit√©rios de Avalia√ß√£o\n",
    "- **Limpeza e Nulos** ‚úîÔ∏è\n",
    "- **Codifica√ß√£o Categ√≥rica** ‚úîÔ∏è\n",
    "- **Explora√ß√£o (EDA) & Visualiza√ß√£o** ‚úîÔ∏è (leve, orientada a vari√°veis)\n",
    "- **Hip√≥teses** ‚úîÔ∏è (em Markdown)\n",
    "- **Sele√ß√£o de Features** ‚úîÔ∏è (opcional por `SelectKBest` e import√¢ncia de features)  \n",
    "- **Modelagem e M√©tricas** ‚úîÔ∏è (accuracy, precision, recall, f1, matriz de confus√£o)\n",
    "- **Ajuste de Hiperpar√¢metros** ‚úîÔ∏è (RandomizedSearchCV ‚Üí GridSearchCV)\n",
    "- **Gera√ß√£o de Submiss√£o** ‚úîÔ∏è (no formato de `sample_submission.csv`)\n",
    "\n",
    "> **Regras**: Apenas `numpy`, `pandas`, `scikit-learn` (modelos), e para gr√°ficos `matplotlib`. Sem bibliotecas externas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b58ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== Imports & Config ==========\n",
    "import os, sys, json, warnings, math, gc, itertools\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualiza√ß√£o (apenas matplotlib conforme regra)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn - preparo e avalia√ß√£o\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sklearn - sele√ß√£o de features (opcional)\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Modelos\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = Path('.')  # Kaggle: mesmo diret√≥rio do notebook\n",
    "print('Python:', sys.version)\n",
    "print('Sklearn:', __import__('sklearn').__version__)\n",
    "print('Files:', [p.name for p in DATA_DIR.iterdir() if p.is_file()][:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541102f",
   "metadata": {},
   "source": [
    "## üì• Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tenta localizar arquivos com nomes padr√£o ou pr√≥ximos\n",
    "candidates_train = ['train.csv', 'Train.csv', 'train_dataset.csv']\n",
    "candidates_test  = ['test.csv', 'Test.csv', 'test_dataset.csv']\n",
    "candidates_sub   = ['sample_submission.csv', 'sample_sub.csv', 'submission_sample.csv']\n",
    "\n",
    "def find_first(paths):\n",
    "    for name in paths:\n",
    "        p = DATA_DIR / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "train_path = find_first(candidates_train)\n",
    "test_path  = find_first(candidates_test)\n",
    "sub_path   = find_first(candidates_sub)\n",
    "\n",
    "if not train_path or not test_path:\n",
    "    raise FileNotFoundError('N√£o encontrei train.csv e/ou test.csv no diret√≥rio. '\n",
    "                            'Coloque ambos no mesmo diret√≥rio do notebook.')\n",
    "\n",
    "print('train_path:', train_path)\n",
    "print('test_path :', test_path)\n",
    "print('sub_path  :', sub_path)\n",
    "\n",
    "df = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "print(df.shape, df_test.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a798d",
   "metadata": {},
   "source": [
    "## üéØ Identificar a vari√°vel alvo (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tentativa autom√°tica de localizar a coluna alvo bin√°ria:\n",
    "# Prefer√™ncias comuns: 'target', 'success', 'label', 'is_success', 'y'\n",
    "common_targets = ['target', 'success', 'label', 'is_success', 'Outcome', 'Status', 'Y', 'y']\n",
    "\n",
    "target = None\n",
    "for c in common_targets:\n",
    "    if c in df.columns:\n",
    "        target = c\n",
    "        break\n",
    "\n",
    "# fallback: se √∫ltima coluna for bin√°ria, usar\n",
    "if target is None:\n",
    "    last_col = df.columns[-1]\n",
    "    if df[last_col].dropna().nunique() <= 2:\n",
    "        target = last_col\n",
    "\n",
    "if target is None:\n",
    "    # procurar qualquer coluna bin√°ria plaus√≠vel\n",
    "    for c in df.columns:\n",
    "        if df[c].dropna().nunique() == 2 and df[c].dtype != 'float64':\n",
    "            target = c\n",
    "            break\n",
    "\n",
    "if target is None:\n",
    "    raise ValueError('N√£o consegui identificar automaticamente a coluna alvo. '\n",
    "                     'Defina manualmente: target = \"nome_da_coluna\"')\n",
    "\n",
    "print('Target:', target)\n",
    "print(df[target].value_counts(normalize=True).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ccbe7",
   "metadata": {},
   "source": [
    "## üîé EDA R√°pida (apenas o essencial para decis√µes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nInfo:')\n",
    "print(df.info())\n",
    "\n",
    "print('\\nNulos (%):')\n",
    "null_pct = df.isna().mean().sort_values(ascending=False)\n",
    "print(null_pct.head(15))\n",
    "\n",
    "# Distribui√ß√£o do alvo\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "df[target].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribui√ß√£o do Alvo')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Contagem')\n",
    "plt.show()\n",
    "\n",
    "# Correla√ß√µes num√©ricas (se existirem)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.drop([target], errors='ignore')\n",
    "if len(num_cols) > 0:\n",
    "    corr = df[num_cols].corr()\n",
    "    # exibe apenas um resumo\n",
    "    print('N√∫mero de colunas num√©ricas:', len(num_cols))\n",
    "else:\n",
    "    print('Sem colunas num√©ricas identificadas (al√©m do target).')\n",
    "\n",
    "# Listar categ√≥ricas\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print('Categ√≥ricas:', len(cat_cols))\n",
    "print(cat_cols[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a71bbb",
   "metadata": {},
   "source": [
    "\n",
    "## üß™ Hip√≥teses\n",
    "1. **Hist√≥rico de financiamento**: maior volume/frqu√™ncia de capta√ß√£o tende a aumentar a probabilidade de sucesso.  \n",
    "2. **Tempo de opera√ß√£o** (idade da startup): empresas com tra√ß√£o de 2‚Äì7 anos podem performar melhor do que muito jovens ou muito antigas (efeito curvil√≠neo).  \n",
    "3. **Localiza√ß√£o/Setor**: hubs (ex.: grandes capitais) e setores com alto investimento (ex.: fintech, sa√∫de) elevam a taxa de sucesso.  \n",
    "> Essas hip√≥teses guiam a aten√ß√£o em features relacionadas a **financiamento**, **tempo** e **contexto** (setor/local).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b17880",
   "metadata": {},
   "source": [
    "## üßπ Pr√©-processamento e Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separar X/y\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target].astype(int) if df[target].dropna().nunique() <= 2 else df[target]\n",
    "\n",
    "# Detecta colunas de tipos\n",
    "num_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Pipelines por tipo\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # √Årvores n√£o precisam de escala; mas √∫til para LR e alguns boosts cl√°ssicos:\n",
    "    # ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, num_features),\n",
    "        ('cat', categorical_pipeline, cat_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Hold-out para relat√≥rio final + CV interno para tuning\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print('Shapes:', X_train.shape, X_valid.shape)\n",
    "print('Positiva (train):', y_train.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3344ebb",
   "metadata": {},
   "source": [
    "## üß© (Opcional) Sele√ß√£o de Features ‚Äî `SelectKBest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_FEATURE_SELECTION = False  # altere para True para ativar\n",
    "\n",
    "kbest_step = ('kbest', SelectKBest(score_func=mutual_info_classif, k='all')) if USE_FEATURE_SELECTION else None\n",
    "kbest_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca8566",
   "metadata": {},
   "source": [
    "## ü§ñ Modelos & Espa√ßos de Busca de Hiperpar√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipelines base\n",
    "def make_pipeline(model):\n",
    "    steps = [('prep', preprocess)]\n",
    "    if kbest_step:\n",
    "        steps.append(kbest_step)\n",
    "    steps.append(('model', model))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "# Modelos\n",
    "rf_model  = RandomForestClassifier(random_state=SEED, n_jobs=-1)\n",
    "gb_model  = GradientBoostingClassifier(random_state=SEED)\n",
    "hgb_model = HistGradientBoostingClassifier(random_state=SEED)\n",
    "\n",
    "# Espa√ßos (coarse) para RandomizedSearchCV\n",
    "rf_dist = {\n",
    "    'model__n_estimators': [200, 400, 600, 800, 1000],\n",
    "    'model__max_depth': [None, 6, 8, 10, 12, 16],\n",
    "    'model__min_samples_split': [2, 5, 10, 20],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 8],\n",
    "    'model__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "gb_dist = {\n",
    "    'model__n_estimators': [100, 200, 400, 600],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'model__max_depth': [2, 3, 4, 5],\n",
    "    'model__subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "hgb_dist = {\n",
    "    'model__max_depth': [None, 6, 8, 10],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__max_leaf_nodes': [15, 31, 63, None],\n",
    "    'model__min_samples_leaf': [10, 20, 30, 50],\n",
    "    'model__l2_regularization': [0.0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "search_spaces = {\n",
    "    'RandomForest': (make_pipeline(rf_model), rf_dist),\n",
    "    'GradientBoosting': (make_pipeline(gb_model), gb_dist),\n",
    "    'HistGradientBoosting': (make_pipeline(hgb_model), hgb_dist)\n",
    "}\n",
    "\n",
    "scoring = {'acc': 'accuracy', 'prec': 'precision', 'rec': 'recall', 'f1': 'f1'}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a42b9",
   "metadata": {},
   "source": [
    "## üîç RandomizedSearchCV (busca ampla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c32ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coarse_results = {}\n",
    "\n",
    "for name, (pipe, dist) in search_spaces.items():\n",
    "    print(f'\\n=== {name} ‚Äî RandomizedSearchCV (coarse) ===')\n",
    "    rand = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=dist,\n",
    "        n_iter=20,  # aumentar se houver tempo\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        random_state=SEED,\n",
    "        verbose=1\n",
    "    )\n",
    "    rand.fit(X_train, y_train)\n",
    "    best_est = rand.best_estimator_\n",
    "    best_acc = rand.best_score_\n",
    "    print('Best CV acc:', round(best_acc, 4))\n",
    "    print('Best params:', rand.best_params_)\n",
    "    coarse_results[name] = {'best_estimator': best_est, 'best_score': best_acc, 'best_params': rand.best_params_}\n",
    "\n",
    "coarse_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa257227",
   "metadata": {},
   "source": [
    "## üéØ GridSearchCV (refino local em torno do melhor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "refined_results = {}\n",
    "\n",
    "for name, res in coarse_results.items():\n",
    "    best_params = res['best_params']\n",
    "    # Construir pequena grade ao redor do melhor\n",
    "    grid = {}\n",
    "    for k, v in best_params.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            # varia√ß√µes locais (+/-) quando num√©rico\n",
    "            around = sorted(set([v] + [v*0.5 if isinstance(v, (int,float)) and v not in (0,1) else v,\n",
    "                                        v*1.5 if isinstance(v, (int,float)) and v not in (0,1) else v]))\n",
    "            # manter inteiros se eram inteiros\n",
    "            if isinstance(v, int):\n",
    "                around = sorted({int(round(x)) for x in around if int(round(x))>0})\n",
    "            grid[k] = list(around)[:3] if len(around)>1 else [v]\n",
    "        else:\n",
    "            # se categ√≥rico, mant√©m o valor\n",
    "            grid[k] = [v]\n",
    "\n",
    "    print(f'\\n=== {name} ‚Äî GridSearchCV (refine) ===')\n",
    "    base_pipe = search_spaces[name][0].set_params(**best_params)\n",
    "    gs = GridSearchCV(\n",
    "        estimator=base_pipe,\n",
    "        param_grid=grid,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        verbose=1\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    refined_results[name] = {\n",
    "        'best_estimator': gs.best_estimator_,\n",
    "        'best_score': gs.best_score_,\n",
    "        'best_params': gs.best_params_\n",
    "    }\n",
    "    print('Refined CV acc:', round(gs.best_score_, 4))\n",
    "    print('Params:', gs.best_params_)\n",
    "\n",
    "refined_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07870252",
   "metadata": {},
   "source": [
    "## üèÅ Compara√ß√£o e Valida√ß√£o no Hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ba91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleciona o melhor por CV\n",
    "best_name = max(refined_results, key=lambda k: refined_results[k]['best_score'])\n",
    "best_est = refined_results[best_name]['best_estimator']\n",
    "best_cv  = refined_results[best_name]['best_score']\n",
    "\n",
    "print('Melhor modelo (CV):', best_name, '‚Äî', round(best_cv, 4))\n",
    "\n",
    "# Avalia no hold-out\n",
    "best_est.fit(X_train, y_train)\n",
    "pred = best_est.predict(X_valid)\n",
    "acc = accuracy_score(y_valid, pred)\n",
    "prec = precision_score(y_valid, pred, zero_division=0)\n",
    "rec = recall_score(y_valid, pred, zero_division=0)\n",
    "f1 = f1_score(y_valid, pred, zero_division=0)\n",
    "\n",
    "print('\\nHold-out:')\n",
    "print('accuracy:', round(acc, 4))\n",
    "print('precision:', round(prec, 4))\n",
    "print('recall   :', round(rec, 4))\n",
    "print('f1       :', round(f1, 4))\n",
    "\n",
    "cm = confusion_matrix(y_valid, pred)\n",
    "fig = plt.figure(figsize=(4,3))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Matriz de Confus√£o (hold-out)')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(np.unique(y)))\n",
    "plt.xticks(tick_marks, tick_marks)\n",
    "plt.yticks(tick_marks, tick_marks)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Verdadeiro')\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_valid, pred, zero_division=0))\n",
    "\n",
    "# Guarda o melhor pipeline treinado completo\n",
    "final_model = best_est\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be380b",
   "metadata": {},
   "source": [
    "## üî¨ Import√¢ncia de Features (quando dispon√≠vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a195c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_names(preprocessor, num_cols, cat_cols):\n",
    "    # ap√≥s o fit, podemos recuperar as colunas do OneHot\n",
    "    # Aten√ß√£o: dependendo da vers√£o do sklearn, o atributo √© get_feature_names_out\n",
    "    out = []\n",
    "    if 'num' in dict(preprocessor.named_transformers_):\n",
    "        out += num_cols\n",
    "    if 'cat' in dict(preprocessor.named_transformers_):\n",
    "        ohe = preprocessor.named_transformers_['cat'].named_steps.get('ohe', None)\n",
    "        if ohe is not None:\n",
    "            try:\n",
    "                ohe_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "            except:\n",
    "                ohe_names = [f'ohe_{i}' for i in range(len(cat_cols))]\n",
    "            out += ohe_names\n",
    "    return out\n",
    "\n",
    "try:\n",
    "    # Extrai nomes ap√≥s o fit\n",
    "    feature_names = get_feature_names(final_model.named_steps['prep'], \n",
    "                                      num_features, cat_features)\n",
    "    model_step = final_model.named_steps['model']\n",
    "    importances = None\n",
    "\n",
    "    if hasattr(model_step, 'feature_importances_'):\n",
    "        importances = model_step.feature_importances_\n",
    "    elif hasattr(model_step, 'coef_'):\n",
    "        coef = model_step.coef_\n",
    "        importances = np.abs(coef[0]) if coef.ndim > 1 else np.abs(coef)\n",
    "\n",
    "    if importances is not None and len(importances) == len(feature_names):\n",
    "        imp_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "        imp_df = imp_df.sort_values('importance', ascending=False).head(25)\n",
    "        display(imp_df)\n",
    "        fig = plt.figure(figsize=(7,6))\n",
    "        plt.barh(imp_df['feature'][::-1], imp_df['importance'][::-1])\n",
    "        plt.title('Top 25 Import√¢ncias de Features')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Modelo n√£o fornece import√¢ncias diretamente ou tamanho divergiu.')\n",
    "except Exception as e:\n",
    "    print('Falha ao calcular import√¢ncias:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ca994",
   "metadata": {},
   "source": [
    "## üì¶ Treinar no conjunto completo & prever `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-treina com todo o treino para extrair previs√µes do teste\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Prepara df_test com mesmas colunas (preprocess garante)\n",
    "test_pred = final_model.predict(df_test)\n",
    "\n",
    "# Monta submiss√£o com base no sample_submission se dispon√≠vel\n",
    "if sub_path and Path(sub_path).exists():\n",
    "    sub = pd.read_csv(sub_path)\n",
    "    # Tenta inferir a coluna de ID e a coluna de target da submiss√£o\n",
    "    sub_cols = sub.columns.tolist()\n",
    "    # Se houver 2 colunas, assume [id, target]\n",
    "    if len(sub_cols) == 2:\n",
    "        id_col, target_col = sub_cols[0], sub_cols[1]\n",
    "        if id_col in df_test.columns:\n",
    "            sub[target_col] = test_pred\n",
    "            submission = sub[[id_col, target_col]]\n",
    "        else:\n",
    "            # fallback: cria um √≠ndice\n",
    "            sub[id_col] = np.arange(len(df_test))\n",
    "            sub[target_col] = test_pred\n",
    "            submission = sub[[id_col, target_col]]\n",
    "    else:\n",
    "        # fallback gen√©rico\n",
    "        submission = pd.DataFrame({\n",
    "            'id': np.arange(len(df_test)),\n",
    "            'target': test_pred\n",
    "        })\n",
    "else:\n",
    "    # fallback sem sample\n",
    "    submission = pd.DataFrame({\n",
    "        'id': np.arange(len(df_test)),\n",
    "        'target': test_pred\n",
    "    })\n",
    "\n",
    "out_path = Path('/mnt/data/submission_startup_success.csv')\n",
    "submission.to_csv(out_path, index=False)\n",
    "out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9aa7e",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Pr√≥ximos Passos e Notas\n",
    "- Caso a acur√°cia **n√£o** atinja 0.80:\n",
    "  - Aumente `n_iter` no `RandomizedSearchCV` (por ex., 50‚Äì100) e densifique o espa√ßo de busca.\n",
    "  - Ajuste `class_weight='balanced'` (para RF) se o alvo estiver desbalanceado.\n",
    "  - Verifique *leaks* e a consist√™ncia entre `train` e `test` (mesma distribui√ß√£o de categorias).\n",
    "  - Ative `USE_FEATURE_SELECTION = True` e teste valores de `k`.\n",
    "- Se o tempo permitir, rode 10-fold CV.\n",
    "- Registre a vers√£o do `scikit-learn` e *seed* para reprodutibilidade no relat√≥rio.\n",
    "- Descreva brevemente porque o modelo vencedor faz sentido (ex.: √°rvores/boosting lidam bem com intera√ß√µes e n√£o linearidades).\n",
    "\n",
    "**Boa sorte no Kaggle!** üí™\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
